---
title: "R Notebook"
output: html_notebook
---

```{r}
library(rgdal)
library(gdalUtils)
library(sf)
library(terra)


download_soilgrids_data <- function(
  voi,          # Variable of interest (e.g., "soc")
  depth,        # Depth slice (e.g., "0-5cm")
  quantile,     # Quantile (e.g., "Q0.5", "mean")
  lrc_long,     # Longitude of the lower-right corner
  lrc_lat,      # Latitude of the lower-right corner
  size,         # Tile size in degrees
  dest.dir      # Destination directory for the output files
) {
  # Load necessary libraries
  options("rgdal_show_exportToProj4_warnings" = "none")
  library(rgdal)
  library(gdalUtils)
  library(sf)
  library(terra)
  
  # Define Coordinate Reference System (CRS)
  crs.igh <- '+proj=igh +lat_0=0 +lon_0=0 +datum=WGS84 +units=m +no_defs'
  
  # Valid options for variables, depths, and quantiles
  quantile.list <- c("Q0.05", "Q0.5", "Q0.95", "mean")
  voi.list.sg <- c("clay", "silt", "sand", "phh2o", "cec", "soc",
                   "bdod", "cfvo", "nitrogen", "ocd")
  depth.list <- paste0(c("0-5", "5-15", "15-30", "30-60",
                         "60-100", "100-200"), "cm")
  
  # Input validation
  if (!(voi %in% voi.list.sg)) {
    stop(paste("Invalid variable of interest. Choose from:",
               paste(voi.list.sg, collapse = ", ")))
  }
  if (!(depth %in% depth.list)) {
    stop(paste("Invalid depth. Choose from:",
               paste(depth.list, collapse = ", ")))
  }
  if (!(quantile %in% quantile.list)) {
    stop(paste("Invalid quantile. Choose from:",
               paste(quantile.list, collapse = ", ")))
  }
  
  # Construct the layer name
  voi_layer <- paste(voi, depth, quantile, sep = "_")
  
  # Define the Area of Interest (AOI)
  tile.lrc <- c(lrc_long, lrc_lat)              # Lower-right corner
  tile.ulc <- c(tile.lrc[1] - size, tile.lrc[2] + size)  # Upper-left corner
  coords <- matrix(c(
    tile.ulc[1], tile.ulc[2],  # Upper-left corner
    tile.lrc[1], tile.ulc[2],  # Upper-right corner
    tile.lrc[1], tile.lrc[2],  # Lower-right corner
    tile.ulc[1], tile.lrc[2],  # Lower-left corner
    tile.ulc[1], tile.ulc[2]   # Close the polygon
  ), ncol = 2, byrow = TRUE)
  
  bb.ll <- st_polygon(list(coords))
  bb.ll <- st_sfc(bb.ll, crs = 4326)
  
  # Transform bounding box to the target CRS
  bb.igh <- st_transform(bb.ll, crs = crs.igh)
  bb.igh.coords <- st_coordinates(bb.igh)[, 1:2]
  bb.sg <- c(
    min(bb.igh.coords[, "X"]),
    max(bb.igh.coords[, "Y"]),
    max(bb.igh.coords[, "X"]),
    min(bb.igh.coords[, "Y"])
  )
  
  # Create a directory prefix based on AOI
  AOI.dir.prefix <- paste0(
    "lat", tile.lrc[2], "_", tile.ulc[2],
    "_lon", tile.ulc[1], "_", tile.lrc[1]
  )
  
  # Set up destination directories
  dest.dir.sg <- file.path(
    dest.dir,
    AOI.dir.prefix,
    voi,
    quantile,
    depth
  )
  if (!dir.exists(dest.dir.sg)) {
    dir.create(dest.dir.sg, recursive = TRUE)
  }
  
  # SoilGrids data URL
  sg_url <- "/vsicurl/https://files.isric.org/soilgrids/latest/data/"
  
  # Define output file paths
  file.out.vrt <- file.path(dest.dir.sg, paste0(voi_layer, ".vrt"))
  file.out.tif <- file.path(dest.dir.sg, paste0(voi_layer, ".tif"))
  
  # Download the VRT file
  gdal_translate(
    src_dataset = paste0(sg_url, voi, "/", voi_layer, ".vrt"),
    dst_dataset = file.out.vrt,
    tr = c(250, 250),
    projwin = bb.sg,
    projwin_srs = crs.igh,
    of = "VRT",
    overwrite = TRUE,
    verbose = TRUE
  )
  
  # Convert the VRT to a GeoTIFF file
  gdal_translate(
    src_dataset = file.out.vrt,
    dst_dataset = file.out.tif,
    co = c("TILED=YES", "COMPRESS=DEFLATE", "PREDICTOR=2", "BIGTIFF=YES"),
    projwin = bb.sg,
    overwrite = TRUE,
    of = "GTiff"
  )
  
  message("Download completed: ", file.out.tif)
}


download_polaris_data <- function(
  voi,          # Variable of interest (e.g., "clay")
  depth,        # Depth slice (e.g., "0_5")
  quantile,     # Quantile (e.g., "p5", "mean")
  lrc_long,     # Longitude of the lower-right corner
  lrc_lat,      # Latitude of the lower-right corner
  dest.dir      # Destination directory for the output files
) {
  # Load necessary libraries
  if (!requireNamespace("terra", quietly = TRUE)) {
    install.packages("terra")
  }
  library(terra)
  
  # Valid options for variables, depths, and quantiles
  quantile.list.polaris <- c("p5", "p50", "p95", "mean")
  voi.list.polaris <- c("clay", "silt", "sand", "ph", "om", "bd")
  depth.list.polaris <- c("0_5", "5_15", "15_30", "30_60", "60_100", "100_200")
  
  # Input validation
  if (!(voi %in% voi.list.polaris)) {
    stop(paste("Invalid variable of interest. Choose from:",
               paste(voi.list.polaris, collapse = ", ")))
  }
  if (!(depth %in% depth.list.polaris)) {
    stop(paste("Invalid depth. Choose from:",
               paste(depth.list.polaris, collapse = ", ")))
  }
  if (!(quantile %in% quantile.list.polaris)) {
    stop(paste("Invalid quantile. Choose from:",
               paste(quantile.list.polaris, collapse = ", ")))
  }
  
  # Construct the layer name
  voi_layer <- paste(voi, depth, quantile, sep = "_")
  
  # Define the Area of Interest (AOI)
  tile.lrc <- c(lrc_long, lrc_lat)              # Lower-right corner
  tile.ulc <- c(tile.lrc[1] - 1, tile.lrc[2] + 1)  # Upper-left corner (since tiles are 1° x 1°)
  
  # Create a directory prefix based on AOI
  AOI.dir.prefix <- paste0(
    "lat", tile.lrc[2], tile.ulc[2],
    "_lon", tile.ulc[1], tile.lrc[1]
  )
  
  # Set up destination directories
  dest.dir.polaris <- file.path(
    dest.dir,
    AOI.dir.prefix,
    voi,
    quantile,
    depth
  )
  if (!dir.exists(dest.dir.polaris)) {
    dir.create(dest.dir.polaris, recursive = TRUE)
  }
  
  # Build the tile file name
  polaris.tile <- paste0(
    "lat", tile.lrc[2], tile.ulc[2],
    "_lon", tile.ulc[1], tile.lrc[1],
    ".tif"
  )
  
  # Build the URL for downloading
  url <- paste0(
    "http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/",
    voi, "/", quantile, "/", depth, "/",
    polaris.tile
  )
  
  # Destination file path
  dest.file <- file.path(dest.dir.polaris, polaris.tile)
  
  # Download the file if it doesn't already exist
  if (!file.exists(dest.file)) {
    message("Downloading POLARIS data from: ", url)
    download.file(
      url = url,
      destfile = dest.file,
      method = "auto",
      mode = "wb"
    )
    message("Download completed: ", dest.file)
  } else {
    message("Local copy of file already exists at: ", dest.file)
  }
}


```


```{r}
# SoilGrids setup
# Define parameters for Coweeta Hydrologic Laboratory
voi <- "clay"          # Variable of interest (e.g., "soc" for soil organic carbon)
depth <- "0-5cm"      # Depth slice (e.g., "0-5cm")
quantile <- "mean"    # Quantile (e.g., "mean")
lrc_long <- -83   # Longitude of the lower-right corner
lrc_lat <- 35     # Latitude of the lower-right corner
size <- 0.5          # Tile size in degrees
dest.dir <- here::here("data/")  # Replace with your actual directory

# Call the function to download data
download_soilgrids_data(
  voi = voi,
  depth = depth,
  quantile = quantile,
  lrc_long = lrc_long,
  lrc_lat = lrc_lat,
  size = size,
  dest.dir = dest.dir
)

sg_clay <- rast(here::here("data/lat35_35.5_lon-83.5_-83/clay/mean/0-5cm/clay_0-5cm_mean.tif"))

# POLARIS setup
# Define parameters
voi <- "clay"
depth <- "0_5"
quantile <- "mean"
lrc_long <- -83   # Longitude of the lower-right corner
lrc_lat <- 35     # Latitude of the lower-right corner
dest.dir <- here::here("data/")  # Replace with your actual directory

# Call the function
download_polaris_data(
  voi = voi,
  depth = depth,
  quantile = quantile,
  lrc_long = lrc_long,
  lrc_lat = lrc_lat,
  dest.dir = dest.dir
)
polaris_clay <- rast(here::here("data/lat3536_lon-84-83/clay/mean/0_5/lat3536_lon-84-83.tif"))


plot(polaris_clay)
plot(sg_clay)



r <- rast(here('data/grids/rss_utm.tif'))
o <- vect('data/vect/Coweeta_Hydrologic_Laboratory.shp')

polaris_clay_proj <- terra::project(polaris_clay, crs(r))
sg_clay_proj <- terra::project(sg_clay, crs(r))

sg_clay_proj_crop <- crop(sg_clay_proj, ext(r))
sg_clay_proj <- mask(sg_clay_proj_crop,  o)

polaris_clay_proj_crop <- crop(polaris_clay_proj, ext(r))
polaris_clay_proj <- mask(polaris_clay_proj_crop,  o)

plot(polaris_clay_proj)
plot(sg_clay_proj/10)
plot(ss['claytotal_r'])
plot(rr['claytotal_r'])
```
```{r}
# Load necessary libraries
library(terra)
library(ggplot2)
library(viridis)
library(cowplot)

# Convert rasters to data frames with explicit renaming
polaris_df <- as.data.frame(polaris_clay_proj, xy = TRUE)
colnames(polaris_df)[3] <- "clay_percent"

sg_df <- as.data.frame(sg_clay_proj, xy = TRUE)
colnames(sg_df)[3] <- "clay_percent"

ss_df <- as.data.frame(ss['claytotal_r'], xy = TRUE)
colnames(ss_df)[3] <- "clay_percent"

rr_df <- as.data.frame(rr['claytotal_r'], xy = TRUE)
colnames(rr_df)[3] <- "clay_percent"

# Set the common color scale range
clay_min <- 9
clay_max <- 25
color_palette <- viridis::viridis(100, option = "C")

# Create individual ggplot maps with updated legend title
plot_polaris <- ggplot(polaris_df) +
  geom_raster(aes(x = x, y = y, fill = clay_percent)) +
  scale_fill_gradientn(name = "Clay %", colors = color_palette, limits = c(clay_min, clay_max)) +
  labs(title = "Polaris Clay") +
  theme_minimal() +
  theme(legend.position = "right")

plot_sg <- ggplot(sg_df) +
  geom_raster(aes(x = x, y = y, fill = clay_percent)) +
  scale_fill_gradientn(name = "Clay %", colors = color_palette, limits = c(clay_min, clay_max)) +
  labs(title = "SoilGrids Clay") +
  theme_minimal() +
  theme(legend.position = "right")

plot_ss <- ggplot(ss_df) +
  geom_raster(aes(x = x, y = y, fill = clay_percent)) +
  scale_fill_gradientn(name = "Clay %", colors = color_palette, limits = c(clay_min, clay_max)) +
  labs(title = "SSURGO Clay") +
  theme_minimal() +
  theme(legend.position = "right")

plot_rr <- ggplot(rr_df) +
  geom_raster(aes(x = x, y = y, fill = clay_percent)) +
  scale_fill_gradientn(name = "Clay %", colors = color_palette, limits = c(clay_min, clay_max)) +
  labs(title = "RSS Clay") +
  theme_minimal() +
  theme(legend.position = "right")

# Extract a shared legend with the new label from one of the plots
shared_legend <- cowplot::get_legend(plot_polaris)

# Remove legends from individual plots
plot_polaris <- plot_polaris + theme(legend.position = "none")
plot_sg <- plot_sg + theme(legend.position = "none")
plot_ss <- plot_ss + theme(legend.position = "none")
plot_rr <- plot_rr + theme(legend.position = "none")

# Arrange the plots in a 2x2 grid and add the shared legend to the right
combined_plot <- cowplot::plot_grid(
  plot_polaris, plot_sg,
  plot_ss, plot_rr,
  ncol = 2, align = "hv", labels = "AUTO"
)

# Add the legend to the right of the grid
final_plot <- cowplot::plot_grid(combined_plot, shared_legend, ncol = 2, rel_widths = c(0.9, 0.1))

# Display the final plot
print(final_plot)

ggsave("clay_maps_grid_with_legend.png", final_plot, width = 12, height = 8, dpi = 300)

```


```{r}
# Uncomment to install packages if needed:
# install.packages(c("jsonlite", "aws.s3"))

library(jsonlite)
library(aws.s3)

# ------------------------------------------------------------------------------
# Helper function to resolve relative URIs against a base URI.
resolve_uri <- function(uri, base_uri) {
  # If the URI is already absolute (starts with http://, https://, or s3://), return it.
  if (grepl("^(http|https|s3)://", uri)) {
    return(uri)
  }
  # Remove any leading "./" or "/" from the relative URI.
  uri <- sub("^\\./", "", uri)
  uri <- sub("^/", "", uri)
  # Remove any trailing slash from base_uri.
  base_uri <- sub("/$", "", base_uri)
  # Combine base_uri and uri.
  absolute_uri <- paste0(base_uri, "/", uri)
  return(absolute_uri)
}

# ------------------------------------------------------------------------------
# Modified read_json_from_uri() that accepts an optional base_uri to resolve relative links.
read_json_from_uri <- function(uri, base_uri = NULL) {
  # If the URI is relative and a base_uri is provided, resolve it.
  if (!grepl("^(http|https|s3)://", uri) && !is.null(base_uri)) {
    uri <- resolve_uri(uri, base_uri)
  }
  
  # If the URI starts with s3://, use aws.s3 to fetch the object.
  if (grepl("^s3://", uri)) {
    # Remove the "s3://" prefix and split into bucket and key.
    uri_no_scheme <- sub("^s3://", "", uri)
    parts <- strsplit(uri_no_scheme, "/", fixed = TRUE)[[1]]
    bucket <- parts[1]
    key <- paste(parts[-1], collapse = "/")
    
    # Retrieve the object from S3 (ensure your AWS credentials are set up).
    obj <- aws.s3::get_object(object = key, bucket = bucket)
    json_text <- rawToChar(obj)
    return(fromJSON(json_text, simplifyVector = FALSE))
  } else {
    # Otherwise, assume it's an HTTP/HTTPS URL.
    return(fromJSON(uri, simplifyVector = FALSE))
  }
}

# ------------------------------------------------------------------------------
# Recursive function to walk through the STAC catalog.
# It collects items (nodes with "type" == "Feature") and passes along the parent's title.
walk_stac <- function(node, parent_title = NULL, base_uri = NULL) {
  items <- list()
  
  # If this node is a STAC item (typically with "type": "Feature")
  if (!is.null(node$type) && node$type == "Feature") {
    node$parent_title <- parent_title
    items[[length(items) + 1]] <- node
  }
  
  # If the node has links, follow those with rel "child" or "item"
  # but only if the link's href ends in ".json" (i.e. points to a catalog/item JSON).
  if (!is.null(node$links)) {
    for (link in node$links) {
      if (!is.null(link$rel) && link$rel %in% c("child", "item")) {
        href <- link$href
        # Only follow this link if it ends with ".json"
        if (!grepl("\\.json$", href)) {
          next
        }
        # Resolve the link if necessary.
        if (!grepl("^(http|https|s3)://", href) && !is.null(base_uri)) {
          href <- resolve_uri(href, base_uri)
        }
        # Compute a new base URI from the directory of the href.
        new_base_uri <- dirname(href)
        # Read the child node.
        child_node <- read_json_from_uri(href, base_uri = new_base_uri)
        if (is.null(child_node)) {
          message(sprintf("Skipping link due to read error: %s", href))
          next
        }
        new_parent_title <- if (!is.null(node$title)) node$title else parent_title
        items <- c(items, walk_stac(child_node, new_parent_title, base_uri = new_base_uri))
      }
    }
  }
  return(items)
}

# ------------------------------------------------------------------------------
# Main code

# Define the root catalog URL and the base URI.
root_catalog_url <- "https://isdasoil.s3.amazonaws.com/catalog.json"
# The initial base_uri is taken from the root URL.
base_uri <- "https://isdasoil.s3.amazonaws.com"

# Read the root catalog (passing the base_uri so that relative links can be resolved).
catalog <- read_json_from_uri(root_catalog_url, base_uri = base_uri)

# Walk through the catalog to gather all items.
items <- walk_stac(catalog, base_uri = base_uri)

# Create an empty list to store items by their id.
assets <- list()

# Iterate over each item.
for (item in items) {
  cat("Type:", item$parent_title, "\n")
  
  # Save the item in the assets list keyed by its id.
  assets[[item$id]] <- item
  
  # If the item has an "assets" field, iterate over its assets.
  if (!is.null(item$assets)) {
    for (asset_name in names(item$assets)) {
      asset <- item$assets[[asset_name]]
      # Check if the asset's roles equal exactly ["data"].
      if (!is.null(asset$roles) && length(asset$roles) == 1 && asset$roles[1] == "data") {
        cat("Title:", asset$title, "\n")
        cat("Description:", asset$description, "\n")
        cat("URL:", asset$href, "\n")
        cat("------------\n")
      }
    }
  }
}


```



```{r}
# Install required packages if needed:
# install.packages(c("terra", "sf"))

library(terra)
library(sf)

# ------------------------------------------------------------------------------
# Define bounding box coordinates (given as lat, lon)
# Upper left corner (lat, lon) and lower right corner (lat, lon)
start_lat_lon <- c(-1.7622, 29.7138)  # upper left: latitude, longitude
end_lat_lon   <- c(-1.7897, 29.7419)  # lower right: latitude, longitude

# The URL for the GeoTIFF file (for example, the pH band)
URL <- "https://isdasoil.s3.amazonaws.com/soil_data/ph/ph.tif"

# ------------------------------------------------------------------------------
# Function to retrieve a subset of the GeoTIFF based on a bounding box.
get_data_subset <- function(start_lat_lon, end_lat_lon, file_location) {
  # If the file is an HTTP/HTTPS URL, add the /vsicurl/ prefix.
  if (grepl("^https://", file_location)) {
    file_location <- paste0("/vsicurl/", file_location)
  }
  # Now open the raster.
  r <- rast(file_location)
  
  # In our inputs the coordinates are provided as (lat, lon).
  # When creating a bounding box for spatial operations the order is (xmin, ymin, xmax, ymax)
  # and coordinates are expected as (lon, lat).
  lat1 <- start_lat_lon[1]
  lon1 <- start_lat_lon[2]
  lat2 <- end_lat_lon[1]
  lon2 <- end_lat_lon[2]
  
  # Define the bounding box in EPSG:4326 (WGS84)
  bbox <- st_bbox(c(xmin = min(lon1, lon2),
                      ymin = min(lat1, lat2),
                      xmax = max(lon1, lon2),
                      ymax = max(lat1, lat2)),
                  crs = st_crs(4326))
  
  # Convert the bbox to an sf object
  bbox_sf <- st_as_sfc(bbox)
  
  # Transform the bounding box to the CRS of the raster
  target_crs <- crs(r)
  bbox_trans <- st_transform(bbox_sf, crs = target_crs)
  
  # Crop the raster to the transformed bounding box
  cropped <- crop(r, vect(bbox_trans))
  
  # Collect some metadata similar to the Python code
  metadata <- list(
    height = nrow(cropped),
    width  = ncol(cropped),
    count  = nlyr(cropped),
    crs    = crs(cropped),
    extent = ext(cropped),
    res    = res(cropped)
  )
  
  return(list(data = cropped, metadata = metadata))
}

# ------------------------------------------------------------------------------
# Function to plot a single-band raster.
plot_raster <- function(raster_band, title = "Raster") {
  # 'raster_band' is assumed to be a SpatRaster with one layer.
  plot(raster_band, main = title)
}

# ------------------------------------------------------------------------------
# Function to plot a grid (2 x 2) of rasters (e.g. one per band)
plot_grid <- function(raster_data, titles) {
  # Assume 'raster_data' is a multi-layer SpatRaster
  n_layers <- nlyr(raster_data)
  
  # Set up a 2x2 plotting layout.
  old_par <- par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))
  for (i in seq_len(n_layers)) {
    plot(raster_data[[i]], main = titles[i])
  }
  par(old_par)
}

# ------------------------------------------------------------------------------
# Get the data subset using our bounding box and file URL.
result <- get_data_subset(start_lat_lon, end_lat_lon, URL)
data   <- result$data
metadata <- result$metadata

# (Optional) Print metadata to check details of the subset.
print(metadata)

# Plot each band in a 2 x 2 grid.
# Here we assume the raster has 4 bands.
band_titles <- paste("band", 1:nlyr(data))
plot_grid(data, band_titles)

```


```{r}
bands <- assets[["ph"]]$assets[["image"]][["eo:bands"]]
bands <- sapply(bands, function(val) val$description)
print(bands)
plot_grid(data, bands)

```

```{r}
# Extract the band information for the 'ph' asset.
bands_info <- assets[["ph"]]$assets[["image"]][["eo:bands"]]

# Extract the description for each band.
bands <- sapply(bands_info, function(band) band$description)

# Print the band descriptions.
print(bands)
# Expected output:
# [1] "pH, predicted mean at 0-20 cm depth"             
# [2] "pH, predicted mean at 20-50 cm depth"             
# [3] "pH, standard deviation at 0-20 cm depth"          
# [4] "pH, standard deviation at 20-50 cm depth"

# Plot the data with the band labels.
# (Assumes that the variable 'data' contains the raster subset and that plot_grid is defined.)
plot_grid(data, bands)

```

```{r}
# Get the backtransformation specification from the STAC item.
conversion <- assets[["ph"]][["back-transformation"]]
print(conversion)
# Expected output: "x/10"

# Create a list of conversion functions.
# Note: The function for "%3000" explicitly converts the result to integer.
conversion_funcs <- list(
  "x"           = function(x) x,
  "x/10"        = function(x) x / 10,
  "x/100"       = function(x) x / 100,
  "expm1(x/10)" = function(x) expm1(x / 10),
  "%3000"       = function(x) as.integer(x %% 3000)
)

# Apply the backtransformation for the "ph" asset.
transformed_data <- conversion_funcs[[conversion]](data)

# Plot the transformed data with the band labels.
# (Assumes that plot_grid() is defined and works similarly to the Python version.)
plot_grid(transformed_data, bands)

```


